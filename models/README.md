# æ¨¡å‹æ–‡ä»¶ç›®éŒ„

æ­¤ç›®éŒ„ç”¨æ–¼å­˜æ”¾AIæ¨¡å‹æ–‡ä»¶ã€‚

## æ‰€éœ€æ¨¡å‹

### 1. Whisper STT æ¨¡å‹
- **èªªæ˜**: faster-whisperæœƒåœ¨é¦–æ¬¡é‹è¡Œæ™‚è‡ªå‹•ä¸‹è¼‰
- **æ¨¡å‹å¤§å°**: medium (~1.5GB)
- **ç„¡éœ€æ‰‹å‹•ä¸‹è¼‰**

### 2. Qwen2.5 LLM æ¨¡å‹
- **æ¨¡å‹åç¨±**: Qwen2.5-7B-Instruct (GGUF 4-bité‡åŒ–)
- **ä¸‹è¼‰ä¾†æº**: [Hugging Face](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF)
- **æ‰€éœ€æ–‡ä»¶**: `qwen2.5-7b-instruct-q4_k_m.gguf`
- **æ–‡ä»¶å¤§å°**: ç´„ 4.4GBï¼ˆåˆ†å‰²ç‚ºå…©å€‹æ–‡ä»¶ï¼‰
- **é¡¯å­˜éœ€æ±‚**: ç´„ 5-6GB

### ä¸‹è¼‰æ­¥é©Ÿ

#### ğŸŒŸ æ¨è–¦æ–¹æ³•ï¼šä½¿ç”¨è‡ªå‹•ä¸‹è¼‰å·¥å…·

**æœ€ç°¡å–®çš„æ–¹å¼ï¼š** ç›´æ¥é›™æ“Šæœ¬ç›®éŒ„ä¸‹çš„ `download_model.bat`

é€™å€‹å·¥å…·æœƒè‡ªå‹•ï¼š
1. âœ… å®‰è£ Hugging Face CLI
2. âœ… è®“æ‚¨é¸æ“‡æ¨¡å‹ï¼ˆ7B / 3B / 1.5Bï¼‰
3. âœ… è‡ªå‹•ä¸‹è¼‰æ¨¡å‹ï¼ˆæ­£ç¢ºè™•ç†åˆ†å‰²æ–‡ä»¶ï¼‰
4. âœ… è‡ªå‹•æ›´æ–° config.json

**æ¨¡å‹é¸æ“‡å»ºè­°ï¼š**

| æ¨¡å‹ | å¤§å° | é¡¯å­˜ | å“è³ª | é©ç”¨å ´æ™¯ |
|------|------|------|------|----------|
| **Qwen2.5-7B** | 4.4GB | 5-6GB | æœ€ä½³ | **RTX 4070 8GBï¼ˆæ¨è–¦ï¼‰** |
| Qwen2.5-3B | 2.0GB | 2-3GB | è‰¯å¥½ | é¡¯å­˜ä¸è¶³æˆ–è¿½æ±‚é€Ÿåº¦ |
| Qwen2.5-1.5B | 1.0GB | 1-2GB | å°šå¯ | ä½é¡¯å­˜æˆ–å¿«é€Ÿæ¸¬è©¦ |

#### ğŸ“¦ æ‰‹å‹•ä¸‹è¼‰æ–¹æ³•ï¼ˆé€²éšç”¨æˆ¶ï¼‰

**æ–¹æ³• 1ï¼šä½¿ç”¨ Hugging Face CLIï¼ˆæ¨è–¦ï¼‰**

```bash
# å®‰è£ CLI
pip install -U "huggingface_hub[cli]"

# ä¸‹è¼‰ 7B æ¨¡å‹
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-GGUF qwen2.5-7b-instruct-q4_k_m.gguf --local-dir . --local-dir-use-symlinks False

# æˆ–ä¸‹è¼‰ 3B æ¨¡å‹
huggingface-cli download Qwen/Qwen2.5-3B-Instruct-GGUF qwen2.5-3b-instruct-q4_k_m.gguf --local-dir . --local-dir-use-symlinks False

# æˆ–ä¸‹è¼‰ 1.5B æ¨¡å‹
huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct-GGUF qwen2.5-1.5b-instruct-q4_k_m.gguf --local-dir . --local-dir-use-symlinks False
```

**æ–¹æ³• 2ï¼šæ‰‹å‹•åˆä½µåˆ†å‰²æ–‡ä»¶**

å¦‚æœä¸‹è¼‰äº†åˆ†å‰²æ–‡ä»¶ï¼Œéœ€è¦ä½¿ç”¨ `llama-gguf-split` å·¥å…·åˆä½µï¼š

```bash
# ä½¿ç”¨ llama-gguf-split åˆä½µï¼ˆå®˜æ–¹æ–¹æ³•ï¼‰
llama-gguf-split --merge qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf qwen2.5-7b-instruct-q4_k_m.gguf
```

æˆ–ç›´æ¥åŸ·è¡Œæä¾›çš„æ‰¹æ¬¡æ–‡ä»¶ï¼š
```
é›™æ“Š models/merge_with_llama_tool.bat
```

**ç²å– llama-gguf-split å·¥å…·ï¼š**
- å¾ [llama.cpp releases](https://github.com/ggerganov/llama.cpp/releases) ä¸‹è¼‰
- æˆ–å®‰è£ï¼š`pip install llama-cpp-python`ï¼ˆåŒ…å«æ­¤å·¥å…·ï¼‰

### æ›¿ä»£æ¨¡å‹ï¼ˆå¦‚æœé¡¯å­˜ä¸è¶³ï¼‰

å¦‚æœ RTX 4070 8GB é¡¯å­˜ä¸è¶³ï¼Œå¯ä»¥ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼š

- **Qwen2.5-3B-Instruct** (Q4_K_M): ~2GB é¡¯å­˜
- **Qwen2.5-1.5B-Instruct** (Q4_K_M): ~1GB é¡¯å­˜

## ç›®éŒ„çµæ§‹

```
models/
â”œâ”€â”€ README.md                              # æœ¬æ–‡ä»¶
â”œâ”€â”€ qwen2.5-7b-instruct-q4_k_m.gguf       # LLMæ¨¡å‹ï¼ˆéœ€ä¸‹è¼‰ï¼‰
â””â”€â”€ .gitkeep
```

æ³¨æ„ï¼šæ¨¡å‹æ–‡ä»¶è¼ƒå¤§ï¼Œä¸æ‡‰æäº¤åˆ°Gitå€‰åº«ã€‚

